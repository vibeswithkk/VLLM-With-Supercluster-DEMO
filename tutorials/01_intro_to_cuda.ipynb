{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce562ce",
   "metadata": {},
   "source": [
    "# Introduction to CUDA Programming\n",
    "\n",
    "This tutorial introduces the fundamental concepts of CUDA programming, which is essential for understanding high-performance GPU computing used in LLM inference engines.\n",
    "\n",
    "## What is CUDA?\n",
    "\n",
    "CUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model developed by NVIDIA. It allows developers to use NVIDIA GPUs for general-purpose computing tasks beyond graphics rendering.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **Host vs Device**: The CPU and its memory are referred to as the host, while the GPU and its memory are referred to as the device.\n",
    "2. **Kernels**: Functions that run on the GPU.\n",
    "3. **Threads**: The basic unit of execution in CUDA.\n",
    "4. **Blocks**: Groups of threads that can cooperate and share memory.\n",
    "5. **Grids**: Collections of blocks.\n",
    "\n",
    "## Memory Hierarchy\n",
    "\n",
    "CUDA provides different types of memory with varying scope and performance characteristics:\n",
    "- **Global Memory**: Large but slow, accessible by all threads\n",
    "- **Shared Memory**: Fast, shared within a block\n",
    "- **Registers**: Fastest, private to each thread\n",
    "- **Constant Memory**: Cached, read-only memory for constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6066f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(\"CUDA Basics Demonstration\")\n",
    "print(\"========================\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this system\")\n",
    "    print(\"This is expected since we're running on a simple laptop for demonstration purposes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe152f3",
   "metadata": {},
   "source": [
    "## CUDA Thread Hierarchy\n",
    "\n",
    "Understanding the thread hierarchy is crucial for CUDA programming:\n",
    "\n",
    "```\n",
    "Grid (entire computation)\n",
    "├── Block 0\n",
    "│   ├── Thread (0,0,0)\n",
    "│   ├── Thread (0,1,0)\n",
    "│   ├── Thread (1,0,0)\n",
    "│   └── Thread (1,1,0)\n",
    "├── Block 1\n",
    "│   ├── Thread (0,0,0)\n",
    "│   ├── Thread (0,1,0)\n",
    "│   ├── Thread (1,0,0)\n",
    "│   └── Thread (1,1,0)\n",
    "└── Block 2\n",
    "    ├── Thread (0,0,0)\n",
    "    ├── Thread (0,1,0)\n",
    "    ├── Thread (1,0,0)\n",
    "    └── Thread (1,1,0)\n",
    "```\n",
    "\n",
    "Each thread can be identified by its unique indices:\n",
    "- `blockIdx`: Block index within the grid\n",
    "- `threadIdx`: Thread index within the block\n",
    "- `blockDim`: Block dimensions\n",
    "- `gridDim`: Grid dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafe89b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate CUDA thread hierarchy concepts\n",
    "def simulate_cuda_grid(num_blocks, threads_per_block):\n",
    "    print(f\"Simulating CUDA grid with {num_blocks} blocks, {threads_per_block} threads per block\")\n",
    "    print(f\"Total threads: {num_blocks * threads_per_block}\")\n",
    "    \n",
    "    for block_id in range(num_blocks):\n",
    "        print(f\"\\nBlock {block_id}:\")\n",
    "        for thread_id in range(threads_per_block):\n",
    "            global_thread_id = block_id * threads_per_block + thread_id\n",
    "            print(f\"  Thread {thread_id} (global ID: {global_thread_id})\")\n",
    "\n",
    "# Demonstrate with a small example\n",
    "simulate_cuda_grid(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a0ceb7",
   "metadata": {},
   "source": [
    "## Memory Management in CUDA\n",
    "\n",
    "Memory management is critical in CUDA programming. Data must be explicitly transferred between host (CPU) and device (GPU) memory:\n",
    "\n",
    "1. **Allocate device memory**\n",
    "2. **Copy data from host to device**\n",
    "3. **Execute kernel on device**\n",
    "4. **Copy results from device to host**\n",
    "5. **Free device memory**\n",
    "\n",
    "This process is often referred to as the \"CUDA memory management cycle\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee9171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate CUDA memory management concepts\n",
    "class CUDAMemorySimulator:\n",
    "    def __init__(self):\n",
    "        self.host_memory = {}\n",
    "        self.device_memory = {}\n",
    "        self.memory_counter = 0\n",
    "    \n",
    "    def malloc_host(self, size, name=None):\n",
    "        if name is None:\n",
    "            name = f\"host_buffer_{self.memory_counter}\"\n",
    "            self.memory_counter += 1\n",
    "        \n",
    "        self.host_memory[name] = np.zeros(size)\n",
    "        print(f\"Allocated {size} elements on host as '{name}'\")\n",
    "        return name\n",
    "    \n",
    "    def malloc_device(self, size, name=None):\n",
    "        if name is None:\n",
    "            name = f\"device_buffer_{self.memory_counter}\"\n",
    "            self.memory_counter += 1\n",
    "        \n",
    "        self.device_memory[name] = np.zeros(size)\n",
    "        print(f\"Allocated {size} elements on device as '{name}'\")\n",
    "        return name\n",
    "    \n",
    "    def memcpy_host_to_device(self, host_name, device_name):\n",
    "        if host_name in self.host_memory and device_name in self.device_memory:\n",
    "            size = min(len(self.host_memory[host_name]), len(self.device_memory[device_name]))\n",
    "            self.device_memory[device_name][:size] = self.host_memory[host_name][:size]\n",
    "            print(f\"Copied {size} elements from '{host_name}' to '{device_name}'\")\n",
    "    \n",
    "    def memcpy_device_to_host(self, device_name, host_name):\n",
    "        if device_name in self.device_memory and host_name in self.host_memory:\n",
    "            size = min(len(self.device_memory[device_name]), len(self.host_memory[host_name]))\n",
    "            self.host_memory[host_name][:size] = self.device_memory[device_name][:size]\n",
    "            print(f\"Copied {size} elements from '{device_name}' to '{host_name}'\")\n",
    "    \n",
    "    def free(self, name):\n",
    "        if name in self.host_memory:\n",
    "            del self.host_memory[name]\n",
    "            print(f\"Freed host memory '{name}'\")\n",
    "        elif name in self.device_memory:\n",
    "            del self.device_memory[name]\n",
    "            print(f\"Freed device memory '{name}'\")\n",
    "\n",
    "# Demonstrate CUDA memory management cycle\n",
    "print(\"CUDA Memory Management Cycle Simulation\")\n",
    "print(\"=====================================\")\n",
    "\n",
    "simulator = CUDAMemorySimulator()\n",
    "\n",
    "# 1. Allocate host memory\n",
    "host_data = simulator.malloc_host(10, \"input_data\")\n",
    "\n",
    "# Initialize host data\n",
    "simulator.host_memory[host_data] = np.arange(10, dtype=np.float32)\n",
    "print(f\"Initialized host data: {simulator.host_memory[host_data]}\")\n",
    "\n",
    "# 2. Allocate device memory\n",
    "device_data = simulator.malloc_device(10, \"gpu_buffer\")\n",
    "\n",
    "# 3. Copy data from host to device\n",
    "simulator.memcpy_host_to_device(host_data, device_data)\n",
    "print(f\"Device data after copy: {simulator.device_memory[device_data]}\")\n",
    "\n",
    "# 4. Simulate kernel execution (square each element)\n",
    "print(\"\\nSimulating kernel execution (squaring each element)\")\n",
    "simulator.device_memory[device_data] = np.square(simulator.device_memory[device_data])\n",
    "print(f\"Device data after kernel: {simulator.device_memory[device_data]}\")\n",
    "\n",
    "# 5. Copy results back to host\n",
    "result_data = simulator.malloc_host(10, \"output_data\")\n",
    "simulator.memcpy_device_to_host(device_data, result_data)\n",
    "print(f\"Host result data: {simulator.host_memory[result_data]}\")\n",
    "\n",
    "# 6. Free memory\n",
    "simulator.free(host_data)\n",
    "simulator.free(device_data)\n",
    "simulator.free(result_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7084c58d",
   "metadata": {},
   "source": [
    "## CUDA Kernel Example\n",
    "\n",
    "A CUDA kernel is a function that runs on the GPU. Here's a simple example of what a vector addition kernel looks like in CUDA C++:\n",
    "\n",
    "```cpp\n",
    "__global__ void vectorAdd(float *A, float *B, float *C, int N)\n",
    "{\n",
    "    int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    \n",
    "    if (i < N)\n",
    "        C[i] = A[i] + B[i];\n",
    "}\n",
    "```\n",
    "\n",
    "Key points about this kernel:\n",
    "1. `__global__`: Indicates this is a kernel function callable from host code\n",
    "2. `blockDim.x * blockIdx.x + threadIdx.x`: Calculates the global thread index\n",
    "3. Boundary checking: Ensures threads don't access memory outside the array\n",
    "4. Parallel execution: Each thread processes one element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedc18e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a CUDA kernel execution\n",
    "def simulate_vector_add_kernel(a, b):\n",
    "    \"\"\"Simulate a vector addition kernel execution\"\"\"\n",
    "    n = len(a)\n",
    "    c = np.zeros(n)\n",
    "    \n",
    "    print(f\"Simulating vector addition of {n} elements\")\n",
    "    print(f\"A: {a}\")\n",
    "    print(f\"B: {b}\")\n",
    "    \n",
    "    # Simulate parallel execution with threads\n",
    "    threads_per_block = 4\n",
    "    num_blocks = (n + threads_per_block - 1) // threads_per_block\n",
    "    \n",
    "    print(f\"\\nLaunching kernel with {num_blocks} blocks, {threads_per_block} threads per block\")\n",
    "    \n",
    "    for block_id in range(num_blocks):\n",
    "        print(f\"  Block {block_id}:\")\n",
    "        for thread_id in range(threads_per_block):\n",
    "            global_id = block_id * threads_per_block + thread_id\n",
    "            \n",
    "            if global_id < n:\n",
    "                c[global_id] = a[global_id] + b[global_id]\n",
    "                print(f\"    Thread {thread_id}: C[{global_id}] = A[{global_id}] + B[{global_id}] = {a[global_id]} + {b[global_id]} = {c[global_id]}\")\n",
    "    \n",
    "    return c\n",
    "\n",
    "# Demonstrate kernel execution\n",
    "a = np.array([1, 2, 3, 4, 5, 6, 7, 8], dtype=np.float32)\n",
    "b = np.array([10, 20, 30, 40, 50, 60, 70, 80], dtype=np.float32)\n",
    "c = simulate_vector_add_kernel(a, b)\n",
    "print(f\"\\nResult: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf6a653",
   "metadata": {},
   "source": [
    "## Performance Considerations\n",
    "\n",
    "When writing CUDA code, several performance factors need to be considered:\n",
    "\n",
    "1. **Memory Coalescing**: Ensuring threads in a warp access contiguous memory locations\n",
    "2. **Occupancy**: Maximizing the number of active warps per SM\n",
    "3. **Divergence**: Avoiding conditional branches that cause threads in a warp to take different paths\n",
    "4. **Shared Memory Usage**: Efficiently using shared memory to reduce global memory accesses\n",
    "\n",
    "## Summary\n",
    "\n",
    "CUDA programming involves understanding:\n",
    "- The parallel execution model (grids, blocks, threads)\n",
    "- Memory management between host and device\n",
    "- Kernel design for parallel execution\n",
    "- Performance optimization techniques\n",
    "\n",
    "These concepts form the foundation for implementing high-performance GPU kernels used in LLM inference engines like vLLM."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
