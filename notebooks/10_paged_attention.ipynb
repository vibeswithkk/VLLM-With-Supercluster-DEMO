{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaa2bf7d",
   "metadata": {},
   "source": [
    "# Paged Attention Implementation\n",
    "\n",
    "This notebook demonstrates the implementation of paged attention, a key optimization technique used in efficient LLM inference engines like vLLM.\n",
    "\n",
    "## What is Paged Attention?\n",
    "\n",
    "Paged attention is a memory management technique that allows for more efficient use of GPU memory during attention computation. Instead of storing all key and value tensors contiguously in memory, paged attention divides them into fixed-size blocks (pages) that can be stored non-contiguously.\n",
    "\n",
    "## Key Benefits\n",
    "\n",
    "1. **Memory Efficiency**: Reduces memory fragmentation\n",
    "2. **Dynamic Sequence Lengths**: Supports variable-length sequences without padding\n",
    "3. **Better Memory Utilization**: Allows for higher batch sizes\n",
    "4. **Reduced Memory Waste**: Eliminates padding overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5717dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Let's first understand the basic concept with a simple example\n",
    "print(\"Paged Attention Concept Demonstration\")\n",
    "print(\"=====================================\")\n",
    "\n",
    "# Simulate a simple attention mechanism\n",
    "def simple_attention(query, key, value):\n",
    "    # Compute attention scores\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "    # Apply softmax\n",
    "    weights = torch.softmax(scores, dim=-1)\n",
    "    # Compute weighted sum\n",
    "    output = torch.matmul(weights, value)\n",
    "    return output\n",
    "\n",
    "# Create sample data\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "head_dim = 8\n",
    "\n",
    "query = torch.randn(batch_size, seq_len, head_dim)\n",
    "key = torch.randn(batch_size, seq_len, head_dim)\n",
    "value = torch.randn(batch_size, seq_len, head_dim)\n",
    "\n",
    "print(f\"Query shape: {query.shape}\")\n",
    "print(f\"Key shape: {key.shape}\")\n",
    "print(f\"Value shape: {value.shape}\")\n",
    "\n",
    "# Compute attention\n",
    "attention_output = simple_attention(query, key, value)\n",
    "print(f\"Attention output shape: {attention_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7d46b2",
   "metadata": {},
   "source": [
    "## Paged Attention Concept\n",
    "\n",
    "In traditional attention, all key and value tensors for a sequence are stored contiguously in memory. In paged attention, we divide these tensors into fixed-size blocks:\n",
    "\n",
    "```\n",
    "Traditional storage:\n",
    "[K0, K1, K2, K3, K4, K5, K6, K7] - All keys stored consecutively\n",
    "\n",
    "Paged storage:\n",
    "Block 0: [K0, K1, K2, K3]\n",
    "Block 1: [K4, K5, K6, K7]\n",
    "```\n",
    "\n",
    "This allows for more flexible memory management, especially when dealing with variable-length sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560b502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate paged attention concept\n",
    "class PagedAttentionSimulator:\n",
    "    def __init__(self, block_size=4):\n",
    "        self.block_size = block_size\n",
    "        self.blocks = {}\n",
    "        self.block_tables = {}\n",
    "    \n",
    "    def allocate_blocks(self, seq_id, seq_len):\n",
    "        # Calculate number of blocks needed\n",
    "        num_blocks = (seq_len + self.block_size - 1) // self.block_size\n",
    "        \n",
    "        # Allocate block IDs\n",
    "        block_ids = []\n",
    "        for i in range(num_blocks):\n",
    "            block_id = len(self.blocks)\n",
    "            self.blocks[block_id] = {\n",
    "                'keys': np.zeros((self.block_size, 8)),  # 8 is head dimension\n",
    "                'values': np.zeros((self.block_size, 8))\n",
    "            }\n",
    "            block_ids.append(block_id)\n",
    "        \n",
    "        # Store block table for this sequence\n",
    "        self.block_tables[seq_id] = block_ids\n",
    "        \n",
    "        return block_ids\n",
    "    \n",
    "    def store_kv_cache(self, seq_id, keys, values):\n",
    "        block_ids = self.block_tables[seq_id]\n",
    "        \n",
    "        # Store keys and values in blocks\n",
    "        idx = 0\n",
    "        for block_id in block_ids:\n",
    "            block = self.blocks[block_id]\n",
    "            \n",
    "            # Calculate how many elements to store in this block\n",
    "            remaining = len(keys) - idx\n",
    "            to_store = min(remaining, self.block_size)\n",
    "            \n",
    "            # Store keys and values\n",
    "            block['keys'][:to_store] = keys[idx:idx+to_store]\n",
    "            block['values'][:to_store] = values[idx:idx+to_store]\n",
    "            \n",
    "            idx += to_store\n",
    "            \n",
    "            if idx >= len(keys):\n",
    "                break\n",
    "    \n",
    "    def retrieve_kv_cache(self, seq_id):\n",
    "        block_ids = self.block_tables[seq_id]\n",
    "        \n",
    "        # Retrieve all keys and values\n",
    "        all_keys = []\n",
    "        all_values = []\n",
    "        \n",
    "        for block_id in block_ids:\n",
    "            block = self.blocks[block_id]\n",
    "            all_keys.append(block['keys'])\n",
    "            all_values.append(block['values'])\n",
    "        \n",
    "        return np.concatenate(all_keys, axis=0), np.concatenate(all_values, axis=0)\n",
    "\n",
    "# Demonstrate paged attention\n",
    "print(\"\\nPaged Attention Demonstration\")\n",
    "print(\"============================\")\n",
    "\n",
    "simulator = PagedAttentionSimulator(block_size=3)\n",
    "\n",
    "# Simulate two sequences of different lengths\n",
    "seq1_len = 7  # Will need 3 blocks (3+3+1)\n",
    "seq2_len = 4  # Will need 2 blocks (3+1)\n",
    "\n",
    "# Allocate blocks for sequences\n",
    "seq1_blocks = simulator.allocate_blocks(1, seq1_len)\n",
    "seq2_blocks = simulator.allocate_blocks(2, seq2_len)\n",
    "\n",
    "print(f\"Sequence 1 (length {seq1_len}) allocated to blocks: {seq1_blocks}\")\n",
    "print(f\"Sequence 2 (length {seq2_len}) allocated to blocks: {seq2_blocks}\")\n",
    "\n",
    "# Generate sample keys and values\n",
    "seq1_keys = np.random.randn(seq1_len, 8)\n",
    "seq1_values = np.random.randn(seq1_len, 8)\n",
    "seq2_keys = np.random.randn(seq2_len, 8)\n",
    "seq2_values = np.random.randn(seq2_len, 8)\n",
    "\n",
    "# Store in paged format\n",
    "simulator.store_kv_cache(1, seq1_keys, seq1_values)\n",
    "simulator.store_kv_cache(2, seq2_keys, seq2_values)\n",
    "\n",
    "# Retrieve and verify\n",
    "retrieved_seq1_keys, retrieved_seq1_values = simulator.retrieve_kv_cache(1)\n",
    "retrieved_seq2_keys, retrieved_seq2_values = simulator.retrieve_kv_cache(2)\n",
    "\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"Sequence 1 keys match: {np.allclose(seq1_keys, retrieved_seq1_keys[:seq1_len])}\")\n",
    "print(f\"Sequence 1 values match: {np.allclose(seq1_values, retrieved_seq1_values[:seq1_len])}\")\n",
    "print(f\"Sequence 2 keys match: {np.allclose(seq2_keys, retrieved_seq2_keys[:seq2_len])}\")\n",
    "print(f\"Sequence 2 values match: {np.allclose(seq2_values, retrieved_seq2_values[:seq2_len])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19471ec1",
   "metadata": {},
   "source": [
    "## Benefits of Paged Attention\n",
    "\n",
    "1. **Memory Efficiency**: By eliminating the need for padding, paged attention reduces memory waste.\n",
    "2. **Dynamic Batching**: Different sequences can share the same physical memory blocks.\n",
    "3. **Reduced Fragmentation**: Fixed-size blocks help reduce memory fragmentation.\n",
    "4. **Scalability**: Better memory utilization allows for larger batch sizes and longer sequences.\n",
    "\n",
    "## Implementation Considerations\n",
    "\n",
    "When implementing paged attention in CUDA:\n",
    "\n",
    "1. **Block Management**: Efficiently allocate and deallocate memory blocks\n",
    "2. **Block Tables**: Maintain mappings between logical and physical block addresses\n",
    "3. **Kernel Optimization**: Design kernels that can efficiently access non-contiguous memory\n",
    "4. **Memory Coalescing**: Ensure efficient memory access patterns despite non-contiguous storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fabe6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison simulation\n",
    "print(\"\\nPerformance Comparison Simulation\")\n",
    "print(\"==================================\")\n",
    "\n",
    "# Simulate memory usage comparison\n",
    "def calculate_memory_usage(batch_size, avg_seq_len, max_seq_len, head_dim, use_paged=False):\n",
    "    if use_paged:\n",
    "        # Paged attention - only allocate what's needed\n",
    "        total_tokens = batch_size * avg_seq_len\n",
    "        memory_per_token = head_dim * 2 * 4  # 2 for key+value, 4 bytes per float\n",
    "        return total_tokens * memory_per_token\n",
    "    else:\n",
    "        # Traditional attention - allocate for max sequence length\n",
    "        memory_per_sequence = max_seq_len * head_dim * 2 * 4\n",
    "        return batch_size * memory_per_sequence\n",
    "\n",
    "# Example scenario\n",
    "batch_size = 32\n",
    "avg_seq_len = 128\n",
    "max_seq_len = 2048\n",
    "head_dim = 128\n",
    "\n",
    "traditional_memory = calculate_memory_usage(batch_size, avg_seq_len, max_seq_len, head_dim, use_paged=False)\n",
    "paged_memory = calculate_memory_usage(batch_size, avg_seq_len, max_seq_len, head_dim, use_paged=True)\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Average sequence length: {avg_seq_len}\")\n",
    "print(f\"Max sequence length: {max_seq_len}\")\n",
    "print(f\"Head dimension: {head_dim}\")\n",
    "print()\n",
    "print(f\"Traditional attention memory usage: {traditional_memory / (1024**2):.2f} MB\")\n",
    "print(f\"Paged attention memory usage: {paged_memory / (1024**2):.2f} MB\")\n",
    "print(f\"Memory savings: {(traditional_memory - paged_memory) / traditional_memory * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b6c845",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Paged attention is a crucial optimization in modern LLM inference engines that provides significant memory efficiency benefits. By dividing key and value caches into fixed-size blocks, it:\n",
    "\n",
    "1. Reduces memory waste from padding\n",
    "2. Enables more efficient memory utilization\n",
    "3. Supports dynamic batching with variable sequence lengths\n",
    "4. Allows for better scalability to larger batch sizes\n",
    "\n",
    "The implementation requires careful management of block allocation, block tables, and optimized CUDA kernels that can efficiently access non-contiguous memory layouts."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
